import os
import re
from typing import List, Tuple

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import cv2
import lpips
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torchvision.transforms as transforms
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from skimage.metrics import structural_similarity as ssim
from tqdm import tqdm

# Transformers –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
import clip
import tiktoken
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    CLIPModel,
    CLIPProcessor,
)

# Diffusers –∏ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
from diffusers import (
    AutoPipelineForImage2Image,
    DDIMScheduler,
    LCMScheduler,
    StableDiffusion3Pipeline,
    StableDiffusionPipeline,
)
from diffusers.utils import load_image, make_image_grid

# Torch –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥—É–ª–∏
from torch import nn
from torch.nn import functional as F
from torchvision import models

def critic(prompt,tokenizer,model):
  
    base_prompt = f'''You get info about the story: Actions, Location, Characters
                    If everything looks correct, answer: YES, else NO
                    ANSWER ONLY YES OR NO
                    INFO:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a critic. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def story_deepthink(prompt,tokenizer,model):
  
    base_prompt = f'''You've got the text of the story. You should think about it
                    You should in details describe the location and characters of this story
                    And describe the actions of the story.
                    Do it maximum illustrative
                    Story:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a thinker. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def location_des—Åriber(prompt, location, tokenizer, model):
  
    base_prompt = f'''You've got the text of the story.
                    You should describe the {location} in this story this story
                    You can imagine how it looks like
                    Do it briefly
                    Story:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a painter. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def location_finder(prompt,tokenizer,model):
  
    base_prompt = f'''You've got the text of the story.
                    Write a locations in which the actions in the story take place.
                    Be brief as possible and write locations with commas
                    WRITE ONLY LOCATIONS DO NOT WRITE EXTRA WORDS
                    STORY:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a painter. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def location_matcher(prompt, location,tokenizer, model):
  
    base_prompt = f'''You've got the action and locations.
                    Write a location in which this actions may take place.
                    
                    ACTION:{prompt}
                    LOCATIONS:{location}
                    CHOOSE ONLY ONE LOCATION FROM A LIST DO NOT WRITE OTHER WORDS
                    USE WORDS FROM A LIST DO NOT CHANGE THEM
                    '''
    messages = [
        {"role": "system", "content": "You're a writer. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def char_des—Åriber(prompt,tokenizer, model):
  
    base_prompt = f'''You've got the text of the story.
                    You should list the characters
                    Do not use names, descrivbe them in general according their roles
                    Do not use extra words and symbols
                    Story:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a writer. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def describe_story(prompt,tokenizer, model):
  
    base_prompt = f'''You get a story to input.
                      Your task is to write the description for the Characters and the Location.
                      You can imagine them
                      Describe it briefly in details
                      WRITE ONLY DESCRIPTIONS!!!
                      Text to be processed:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a professional writer. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=70
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def generate_response(prompt,tokenizer, model, parts = 3):
  
    base_prompt = f'''  You need to divide the text ONLY into {parts} actions and describe them, ONLY {parts}, NOT MORE!!!!
                        Seperate each main ACTION and begin with token '[ACTION]'
                        Do not use any characters except letters, numbers, and the dot and comma sign.
                        DO NOT USE OTHER TOKENS!!!!
                        THERE ARE MUST BE ONLY {parts} PARTS!!!!
                        Text to be processed:{prompt}'''
    messages = [
        {"role": "system", "content": "You're a professional text writer. You're best in it"},
        {"role": "user", "content": base_prompt}
    ]

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=64
    )

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def clean_text(text):
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤, —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
    cleaned_text = re.sub(r"[^][a-zA-Z0-9 .,]", "", text)
    return cleaned_text

def merge_actions(actions, num=3):
    if len(actions) <= num:
        return actions
    else:
        # –ï—Å–ª–∏ —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–µ 3, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ
        step = len(actions) // num  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∞–≥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merged_actions = [
            " ".join(actions[i:i + step]) for i in range(0, len(actions), step)
        ]
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ—Å—Ç–∞–ª–æ—Å—å –±–æ–ª—å—à–µ 3 —á–∞—Å—Ç–µ–π, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ
        if len(merged_actions) > num:
            merged_actions[2] += " " + " ".join(merged_actions[num:])
            merged_actions = merged_actions[:num]
        return merged_actions

def calculate_clip_score(image: str, text: str, model_name: str = "ViT-B/32") -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç CLIP Score ‚Äî –º–µ—Ä—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Ç–µ–∫—Å—Ç–æ–º.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        image_path (str): –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
        text (str): –¢–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ CLIP (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "ViT-B/32").
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ CLIP Score (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏).
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load(model_name, device=device)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    image = preprocess(image).unsqueeze(0).to(device)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_tokens = clip.tokenize([text]).to(device)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    with torch.no_grad():
        image_features = model.encode_image(image)
        text_features = model.encode_text(text_tokens)
    
    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    clip_score = torch.cosine_similarity(image_features, text_features).item()
    
    return clip_score

def calculate_lpips(image_path1: str, image_path2: str, net_type: str = 'alex') -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫—É LPIPS –º–µ–∂–¥—É –¥–≤—É–º—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        image_path1 (str): –ü—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
        image_path2 (str): –ü—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
        net_type (str): –ú–æ–¥–µ–ª—å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è ('alex', 'vgg', 'squeeze').

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ LPIPS (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –±–æ–ª–µ–µ —Å—Ö–æ–∂–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è).
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LPIPS
    loss_fn = lpips.LPIPS(net=net_type).eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    loss_fn.to(device)

    def load_image(path):
        img = path#Image.open(path).convert('RGB')
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        return transform(img).unsqueeze(0).to(device)

    img1 = load_image(image_path1)
    img2 = load_image(image_path2)

    # –†–∞—Å—á–µ—Ç LPIPS
    with torch.no_grad():
        distance = loss_fn(img1, img2).item()

    return distance

def parse_characters(input_str):
    # –£–¥–∞–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "Characters:" (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
    input_str = re.sub(r'^.*characters:.*$', '', input_str, flags=re.IGNORECASE | re.MULTILINE)
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
    entries = re.findall(r'^\s*-\s*(.*?)\s*$', input_str, flags=re.MULTILINE)
    
    # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    cleaned_entries = []
    for entry in entries:
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∫–æ–±–æ–∫
        entry = re.sub(r'[^\w\s()]', '', entry)
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        entry = re.sub(r'\s+', ' ', entry).strip()
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ Title Case (–ø–µ—Ä–≤—ã–µ –±—É–∫–≤—ã —Å–ª–æ–≤ –∑–∞–≥–ª–∞–≤–Ω—ã–µ)
        entry = entry.title()
        cleaned_entries.append(entry)
    
    return ', '.join(cleaned_entries)

def parse_actions(input_str):
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ [ACTION] –≤ –ª—é–±–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
    actions = re.split(r'\[ACTION\]', input_str, flags=re.IGNORECASE)
    
    # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    cleaned_actions = [action.strip() for action in actions if action.strip()]
    
    return cleaned_actions

def create_story_report(story_ls: list, characters: list, location_ls: list) -> str:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ø–∏—Å–∫–∏ —Å—é–∂–µ—Ç–æ–≤, –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ –ª–æ–∫–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        story_ls: –°–ø–∏—Å–æ–∫ —Å—é–∂–µ—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π
        characters: –°–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (—Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏)
        location_ls: –°–ø–∏—Å–æ–∫ –ª–æ–∫–∞—Ü–∏–π
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    """
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—é–∂–µ—Ç
    report = "üìñ –°–Æ–ñ–ï–¢–ù–´–ï –°–û–ë–´–¢–ò–Ø:\n"
    report += "\n".join(f"‚Üí {i+1}. {story}" for i, story in enumerate(story_ls))
    
    # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
    report += "\n\nüé≠ –ü–ï–†–°–û–ù–ê–ñ–ò:\n"
    if isinstance(characters, dict):  # –ï—Å–ª–∏ characters - —Å–ª–æ–≤–∞—Ä—å
        report += "\n".join(f"‚Ä¢ {name}: {desc}" for name, desc in characters.items())
    else:  # –ï—Å–ª–∏ characters - —Å–ø–∏—Å–æ–∫
        report += "\n".join(f"‚Ä¢ {char}" for char in characters)
    
    # –õ–æ–∫–∞—Ü–∏–∏
    report += "\n\nüåç –õ–û–ö–ê–¶–ò–ò:\n"
    unique_locations = list(set(location_ls))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–æ–∫–∞—Ü–∏–∏
    report += "\n".join(f"üìç {loc}" for loc in unique_locations)
    
    
    return report

def llm_agents(story: str,tokenizer, model) -> Tuple[List[str], dict, List[str]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö story_ls –∏ location_ls —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤."""
    enc = tiktoken.get_encoding("cl100k_base")
    
    def count_tokens(text: str) -> int:
        return len(enc.encode(text))

    # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ç–æ–∫–µ–Ω–æ–≤)
    characters = parse_characters(char_des—Åriber(story,tokenizer, model))
    
    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è story_ls (–º–∞–∫—Å 3 –ø–æ–ø—ã—Ç–∫–∏)
    story_ls = []
    for _ in range(3):
        story_ls = merge_actions(parse_actions(generate_response(story,tokenizer, model)))
        if all(count_tokens(item) < 77 for item in story_ls):
            break
    else:
        story_ls = [s[:70] + "..." for s in story_ls]  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è location_ls (–º–∞–∫—Å 3 –ø–æ–ø—ã—Ç–∫–∏)
    locations_raw = location_finder(story,tokenizer, model)
    location_ls = []
    
    for attempt in range(3):
        location_ls = []
        for story_item in story_ls:
            loc = location_matcher(story_item, locations_raw,tokenizer, model)
            location_ls.append(loc[:70] + "..." if count_tokens(loc) >= 77 else loc)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª–∏–Ω—ã
        if len(location_ls) != len(story_ls):
            diff = len(story_ls) - len(location_ls)
            filler = location_ls[0] if len(set(location_ls)) == len(location_ls) else location_ls[-1]
            location_ls.extend([filler] * diff)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        if (len(location_ls) == len(story_ls) and 
            all(count_tokens(loc) < 77 for loc in location_ls)):
            if 'yes' in critic(create_story_report(story_ls, characters, location_ls),tokenizer, model).lower():
                break
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏
    story_ls = [s if count_tokens(s) < 77 else s[:70] + "..." for s in story_ls]
    location_ls = location_ls[:len(story_ls)]
    location_ls = [loc if count_tokens(loc) < 77 else loc[:70] + "..." 
                  for loc in location_ls]
    
    return story_ls, characters, location_ls

def generate_best_character_image(pipeline, characters: str, generator, empty_image, show_results: bool = False) -> Image.Image:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–µ–µ –ø–æ CLIP Score.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        pipeline: –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        characters (str): –û–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        generator: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        empty_image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è IP-–∞–¥–∞–ø—Ç–µ—Ä–∞
        show_results (bool): –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        Image.Image: –õ—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ CLIP Score
    """
    pipeline.set_ip_adapter_scale(0)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    generated_images = []
    clip_scores = []
    
    for i in range(3):
        char_img = pipeline(
            prompt=f'Illustration {characters}',
            ip_adapter_image=empty_image,
            negative_prompt="lowres, bad anatomy, worst quality, low quality",
            num_inference_steps=50,
            generator=generator,
        ).images[0]
        
        score = calculate_clip_score(char_img, characters)
        generated_images.append(char_img)
        clip_scores.append(score)
        
        if show_results:
            plt.figure(figsize=(5, 5))
            plt.imshow(char_img)
            plt.title(f"–í–∞—Ä–∏–∞–Ω—Ç {i+1} - CLIP: {score:.4f}")
            plt.axis('off')
            plt.show()
    
    # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    best_idx = np.argmax(clip_scores)
    best_image = generated_images[best_idx]
    best_score = clip_scores[best_idx]
    
    print(f"–õ—É—á—à–∏–π CLIP Score: {best_score:.4f} (–≤–∞—Ä–∏–∞–Ω—Ç {best_idx+1})")
    
    if show_results:
        plt.figure(figsize=(6, 6))
        plt.imshow(best_image)
        plt.title(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - CLIP: {best_score:.4f}")
        plt.axis('off')
        plt.show()
    
    return best_image

def generate_backgrounds_with_quality_check(
    pipeline,
    characters: str,
    location_ls: list,
    char_image: Image.Image,
    generator,
    negative_prompt: str = "lowres, bad anatomy, worst quality, low quality",
    min_lpips_quality: float = 0.5,
    max_attempts: int = 3
) -> dict:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ–Ω—ã –¥–ª—è –ª–æ–∫–∞—Ü–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ LPIPS.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        pipeline: –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        characters (str): –û–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        location_ls: –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏–π
        char_image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        generator: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        negative_prompt: –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        min_lpips_quality: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (1 - LPIPS)
        max_attempts: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        dict: –°–ª–æ–≤–∞—Ä—å {–ª–æ–∫–∞—Ü–∏—è: Image} —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º
    """
    pipeline.set_ip_adapter_scale(0.2)
    background_dict = {}
    
    for loc in list(set(location_ls)):
        for attempt in range(max_attempts):
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–æ–Ω–∞
            background = pipeline(
                prompt=f'High-quality illustration of {characters} in {loc}',
                ip_adapter_image=char_image,
                negative_prompt=negative_prompt,
                num_inference_steps=50,
                generator=generator,
            ).images[0]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            lpips_score = calculate_lpips(char_image, background)
            quality = 1 - lpips_score
            
            if quality >= min_lpips_quality:
                background_dict[loc] = background
                print(f"‚úì –õ–æ–∫–∞—Ü–∏—è '{loc}': –∫–∞—á–µ—Å—Ç–≤–æ {quality:.4f} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1})")
                break
                
            print(f"√ó –õ–æ–∫–∞—Ü–∏—è '{loc}': –∫–∞—á–µ—Å—Ç–≤–æ {quality:.4f} < {min_lpips_quality} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1})")
            
            if attempt == max_attempts - 1:
                background_dict[loc] = background  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π –∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                print(f"‚ö† –î–ª—è –ª–æ–∫–∞—Ü–∏–∏ '{loc}' –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫. –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(char_image)
    plt.title("–ü–µ—Ä—Å–æ–Ω–∞–∂")
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    first_loc = next(iter(background_dict))
    plt.imshow(background_dict[first_loc])
    plt.title(f"–§–æ–Ω: {first_loc}\nLPIPS –∫–∞—á–µ—Å—Ç–≤–æ: {1 - calculate_lpips(char_image, background_dict[first_loc]):.4f}")
    plt.axis('off')
    
    plt.show()
    
    return background_dict

def generate_best_story_frames(
    pipeline,
    story_ls: list,
    location_ls: list,
    background_dict: dict,
    characters: str,
    generator,
    negative_prompt: str = "lowres, bad anatomy, worst quality, low quality",
    num_variants: int = 3,
    min_lpips: float = 0.5,
    display: bool = True
) -> list:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ –∫–∞–¥—Ä—ã –¥–ª—è –∏—Å—Ç–æ—Ä–∏–π —Å –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ (CLIP + LPIPS).
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        pipeline: –ü–∞–π–ø–ª–∞–π–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        story_ls: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—Ä–∏–π
        location_ls: –°–ø–∏—Å–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ª–æ–∫–∞—Ü–∏–π
        background_dict: –°–ª–æ–≤–∞—Ä—å —Ñ–æ–Ω–æ–≤ {–ª–æ–∫–∞—Ü–∏—è: Image}
        characters: –û–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        generator: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        negative_prompt: –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        num_variants: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—é
        min_lpips: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ LPIPS (1 - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ)
        display: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        list: –õ—É—á—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∏—Å—Ç–æ—Ä–∏–∏
    """
    pipeline.set_ip_adapter_scale(0.2)
    best_frames = []
    best_clips = []
    best_lpips = []
    
    for i, (story, loc) in enumerate(zip(story_ls, location_ls)):
        top_candidates = []
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        for _ in range(num_variants):
            img = pipeline(
                prompt=f'Illustration: {story} with {characters}',
                ip_adapter_image=background_dict.get(loc),
                negative_prompt=negative_prompt,
                num_inference_steps=50,
                generator=generator,
            ).images[0]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            clip_score = calculate_clip_score(img, story)
            lpips_dist = calculate_lpips(background_dict[loc], img)
            lpips_quality = 1 - lpips_dist
            
            top_candidates.append({
                'image': img,
                'clip': clip_score,
                'lpips': lpips_quality,
                'combined_score': clip_score * (lpips_quality if lpips_quality >= min_lpips else 0)
            })
        
        # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É score
        top_candidates.sort(key=lambda x: x['combined_score'], reverse=True)
        best = top_candidates[0]
        best_frames.append(best['image'])
        best_clips.append(best['clip'])
        best_lpips.append(best['lpips'])
        if display:
            print(f"–ò—Å—Ç–æ—Ä–∏—è {i+1}:")
            print(f"  CLIP: {best['clip']:.4f} | LPIPS: {best['lpips']:.4f}")
            print(f"  –¢–µ–∫—Å—Ç: {story[:50]}...")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if display:
        plt.figure(figsize=(20, 5))
        for idx, img in enumerate(best_frames):
            plt.subplot(1, len(best_frames), idx + 1)
            plt.imshow(img)
            plt.title(f"–ö–∞–¥—Ä {idx+1}\n{story_ls[idx][:30]}...")
            plt.axis("off")
        plt.tight_layout()
        plt.show()
    
    return best_frames, best_clips, best_lpips

def save_images(image_list, num_act, output_dir="output_images", prefix="frame"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    
    :param image_list: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ PIL.Image
    :param output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "output_images")
    :param prefix: –ü—Ä–µ—Ñ–∏–∫—Å –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "frame")
    """
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
    os.makedirs(output_dir, exist_ok=True)
    
    for i, img in enumerate(image_list):
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞: frame_1.jpg, frame_2.jpg –∏ —Ç.–¥.
        filename = f"{prefix}_{str(num_act)}_{i+1}.jpg"
        filepath = os.path.join(output_dir, filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JPEG
        img.save(filepath, "JPEG", quality=95)
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")

def gen_img(text,
           use_lora = True):

    ## LLM
    model_name = "Qwen/Qwen2.5-1.5B-Instruct" 
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cuda").to("cuda")
    
    ## Stable Diffusion
    pipeline = StableDiffusionPipeline.from_pretrained(
        "stable-diffusion-v1-5/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
    ).to("cuda")
    
    pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
    pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
    
    pipeline.set_ip_adapter_scale(0.2)

    if use_lora:
        pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)
        pipeline.load_lora_weights("/kaggle/input/load-lora-weights-pipe-unet-load-attn-procs/out/")
        pipeline.fuse_lora()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    #empty_image = Image.new("RGB", (512, 512), (255, 255, 255))  # –ë–µ–ª—ã–π —Ñ–æ–Ω, —Ä–∞–∑–º–µ—Ä 512x512
    width, height = 512, 512
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—à—É–º) –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 255]
    empty_image = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
    
    generator = torch.Generator(device="cpu").manual_seed(26)

    story_ls, characters, location_ls = llm_agents(stories[i],tokenizer, model)
    
    story_report = create_story_report(
                                    story_ls=story_ls,
                                    characters=characters,
                                    location_ls=location_ls)

    best_char_image = generate_best_character_image(
                                                    pipeline=pipeline,
                                                    characters=characters,
                                                    generator=generator,
                                                    empty_image=empty_image,
                                                    show_results=True)

    background_dict = generate_backgrounds_with_quality_check(
                                                            pipeline=pipeline,
                                                            characters=characters,
                                                            location_ls=location_ls,
                                                            char_image=best_char_image,
                                                            generator=generator,
                                                            min_lpips_quality=0.5)

    best_frames, best_clips, best_lpips = generate_best_story_frames(
                                                                    pipeline=pipeline,
                                                                    story_ls=story_ls,
                                                                    location_ls=location_ls,
                                                                    background_dict=background_dict,
                                                                    characters=characters,
                                                                    generator=generator)
    
    return best_frames, best_clips, best_lpips
        